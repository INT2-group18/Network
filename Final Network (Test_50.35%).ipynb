{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iABXs37PYLuP"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import gc\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "VHdEPmVXbC_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training variables\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 150\n",
        "LEARNING_RATE = 0.00005"
      ],
      "metadata": {
        "id": "WZi6DrrfbEkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations for the train, validation, and test datasets\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256, antialias=True),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256), antialias=True),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the train, validation, and test datasets\n",
        "train_dataset = datasets.Flowers102(root=\"./data\", split=\"train\", transform=train_transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_dataset = datasets.Flowers102(root=\"./data\", split=\"val\", transform=val_test_transform, download=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_dataset = datasets.Flowers102(root=\"./data\", split=\"test\", transform=val_test_transform, download=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "WLXBzxwbbO_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Create an instance of the CNN and move it to the device\n",
        "cnn = CNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Set weight decay\n",
        "WEIGHT_DECAY = 0.03\n",
        "\n",
        "# Create the optimizer and add weight decay\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.1, verbose=True)\n"
      ],
      "metadata": {
        "id": "c3L6lRUEbjFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "                                   nn.BatchNorm2d(16),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.MaxPool2d(2, 2)) # 224 -> 112\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "                                   nn.BatchNorm2d(32),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.MaxPool2d(2, 2)) # 112 -> 56\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "                                   nn.BatchNorm2d(64),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.MaxPool2d(2, 2)) # 56 -> 28\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "                                   nn.BatchNorm2d(128),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.MaxPool2d(2, 2)) # 28 -> 14\n",
        "        self.conv5 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "                                   nn.BatchNorm2d(256),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.MaxPool2d(2, 2)) # 14 -> 7\n",
        "        self.conv6 = nn.Sequential(nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                                   nn.BatchNorm2d(512),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.MaxPool2d(2, 2)) # 7 -> 3\n",
        "        self.conv7 = nn.Sequential(nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "                                   nn.BatchNorm2d(1024),\n",
        "                                   nn.MaxPool2d(2, 2)) # 3 -> 1\n",
        "\n",
        "        self.fc1 = nn.Sequential(nn.Linear(1024, 512),\n",
        "                                 nn.ReLU())\n",
        "        self.fc2 = nn.Sequential(nn.Linear(512, 256),\n",
        "                                 nn.ReLU())\n",
        "\n",
        "        self.fc3 = nn.Sequential(nn.Linear(256, 102))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = x.view(-1, 1024)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "BOahlLyxbt6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yhGoXwwnQiQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d40e63a-7386-46b6-d68f-9283e40941c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 344862509/344862509 [00:24<00:00, 13875396.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/flowers-102/102flowers.tgz to data/flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to data/flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 502/502 [00:00<00:00, 561627.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to data/flowers-102/setid.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14989/14989 [00:00<00:00, 15461982.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/160], Step [16/32], Step Loss: 4.5858\n",
            "Epoch [1/160], Step [32/32], Step Loss: 4.5697\n",
            "Epoch [1/160], Training Loss: 4.5824, Training Accuracy: 1.86%\n",
            "Epoch [1/160], Validation Loss: 4.5647, Validation Accuracy: 2.25%\n",
            "Epoch [2/160], Step [16/32], Step Loss: 4.3865\n",
            "Epoch [2/160], Step [32/32], Step Loss: 4.3287\n",
            "Epoch [2/160], Training Loss: 4.4093, Training Accuracy: 4.61%\n",
            "Epoch [2/160], Validation Loss: 4.3262, Validation Accuracy: 6.47%\n",
            "Epoch [3/160], Step [16/32], Step Loss: 4.2363\n",
            "Epoch [3/160], Step [32/32], Step Loss: 4.2385\n",
            "Epoch [3/160], Training Loss: 4.2157, Training Accuracy: 8.53%\n",
            "Epoch [3/160], Validation Loss: 4.1437, Validation Accuracy: 9.90%\n",
            "Epoch [4/160], Step [16/32], Step Loss: 3.9889\n",
            "Epoch [4/160], Step [32/32], Step Loss: 3.9687\n",
            "Epoch [4/160], Training Loss: 4.0005, Training Accuracy: 12.06%\n",
            "Epoch [4/160], Validation Loss: 3.9344, Validation Accuracy: 12.35%\n",
            "Epoch [5/160], Step [16/32], Step Loss: 3.8648\n",
            "Epoch [5/160], Step [32/32], Step Loss: 3.5534\n",
            "Epoch [5/160], Training Loss: 3.7760, Training Accuracy: 17.84%\n",
            "Epoch [5/160], Validation Loss: 3.7479, Validation Accuracy: 18.04%\n",
            "Epoch [6/160], Step [16/32], Step Loss: 3.4147\n",
            "Epoch [6/160], Step [32/32], Step Loss: 3.6803\n",
            "Epoch [6/160], Training Loss: 3.5297, Training Accuracy: 24.22%\n",
            "Epoch [6/160], Validation Loss: 3.5340, Validation Accuracy: 21.86%\n",
            "Epoch [7/160], Step [16/32], Step Loss: 3.3468\n",
            "Epoch [7/160], Step [32/32], Step Loss: 3.1723\n",
            "Epoch [7/160], Training Loss: 3.3214, Training Accuracy: 27.84%\n",
            "Epoch [7/160], Validation Loss: 3.3906, Validation Accuracy: 23.33%\n",
            "Epoch [8/160], Step [16/32], Step Loss: 3.0744\n",
            "Epoch [8/160], Step [32/32], Step Loss: 2.9217\n",
            "Epoch [8/160], Training Loss: 3.1524, Training Accuracy: 30.10%\n",
            "Epoch [8/160], Validation Loss: 3.2833, Validation Accuracy: 22.84%\n",
            "Epoch [9/160], Step [16/32], Step Loss: 2.7279\n",
            "Epoch [9/160], Step [32/32], Step Loss: 2.7554\n",
            "Epoch [9/160], Training Loss: 2.9301, Training Accuracy: 35.98%\n",
            "Epoch [9/160], Validation Loss: 3.0783, Validation Accuracy: 29.61%\n",
            "Epoch [10/160], Step [16/32], Step Loss: 2.7602\n",
            "Epoch [10/160], Step [32/32], Step Loss: 2.7281\n",
            "Epoch [10/160], Training Loss: 2.7511, Training Accuracy: 41.96%\n",
            "Epoch [10/160], Validation Loss: 2.9961, Validation Accuracy: 30.88%\n",
            "Epoch [11/160], Step [16/32], Step Loss: 2.8595\n",
            "Epoch [11/160], Step [32/32], Step Loss: 2.6784\n",
            "Epoch [11/160], Training Loss: 2.5751, Training Accuracy: 45.20%\n",
            "Epoch [11/160], Validation Loss: 2.8647, Validation Accuracy: 33.82%\n",
            "Epoch [12/160], Step [16/32], Step Loss: 2.6631\n",
            "Epoch [12/160], Step [32/32], Step Loss: 2.5877\n",
            "Epoch [12/160], Training Loss: 2.4186, Training Accuracy: 47.55%\n",
            "Epoch [12/160], Validation Loss: 2.7822, Validation Accuracy: 32.84%\n",
            "Epoch [13/160], Step [16/32], Step Loss: 2.4202\n",
            "Epoch [13/160], Step [32/32], Step Loss: 2.4809\n",
            "Epoch [13/160], Training Loss: 2.3186, Training Accuracy: 49.80%\n",
            "Epoch [13/160], Validation Loss: 2.8287, Validation Accuracy: 32.94%\n",
            "Epoch [14/160], Step [16/32], Step Loss: 1.9893\n",
            "Epoch [14/160], Step [32/32], Step Loss: 2.2274\n",
            "Epoch [14/160], Training Loss: 2.1385, Training Accuracy: 55.00%\n",
            "Epoch [14/160], Validation Loss: 2.7075, Validation Accuracy: 34.61%\n",
            "Epoch [15/160], Step [16/32], Step Loss: 2.1713\n",
            "Epoch [15/160], Step [32/32], Step Loss: 2.4402\n",
            "Epoch [15/160], Training Loss: 2.0486, Training Accuracy: 57.55%\n",
            "Epoch [15/160], Validation Loss: 2.7094, Validation Accuracy: 34.71%\n",
            "Epoch [16/160], Step [16/32], Step Loss: 1.9353\n",
            "Epoch [16/160], Step [32/32], Step Loss: 1.8828\n",
            "Epoch [16/160], Training Loss: 1.9368, Training Accuracy: 58.92%\n",
            "Epoch [16/160], Validation Loss: 2.6153, Validation Accuracy: 36.57%\n",
            "Epoch [17/160], Step [16/32], Step Loss: 2.1419\n",
            "Epoch [17/160], Step [32/32], Step Loss: 1.9328\n",
            "Epoch [17/160], Training Loss: 1.8069, Training Accuracy: 62.55%\n",
            "Epoch [17/160], Validation Loss: 2.5101, Validation Accuracy: 39.71%\n",
            "Epoch [18/160], Step [16/32], Step Loss: 2.0599\n",
            "Epoch [18/160], Step [32/32], Step Loss: 1.7591\n",
            "Epoch [18/160], Training Loss: 1.6981, Training Accuracy: 67.35%\n",
            "Epoch [18/160], Validation Loss: 2.5476, Validation Accuracy: 36.18%\n",
            "Epoch [19/160], Step [16/32], Step Loss: 1.6012\n",
            "Epoch [19/160], Step [32/32], Step Loss: 1.5195\n",
            "Epoch [19/160], Training Loss: 1.5954, Training Accuracy: 69.31%\n",
            "Epoch [19/160], Validation Loss: 2.3902, Validation Accuracy: 42.65%\n",
            "Epoch [20/160], Step [16/32], Step Loss: 1.3126\n",
            "Epoch [20/160], Step [32/32], Step Loss: 1.3913\n",
            "Epoch [20/160], Training Loss: 1.4364, Training Accuracy: 71.67%\n",
            "Epoch [20/160], Validation Loss: 2.4173, Validation Accuracy: 39.80%\n",
            "Epoch [21/160], Step [16/32], Step Loss: 1.4067\n",
            "Epoch [21/160], Step [32/32], Step Loss: 1.4694\n",
            "Epoch [21/160], Training Loss: 1.3389, Training Accuracy: 74.61%\n",
            "Epoch [21/160], Validation Loss: 2.3701, Validation Accuracy: 41.86%\n",
            "Epoch [22/160], Step [16/32], Step Loss: 1.1823\n",
            "Epoch [22/160], Step [32/32], Step Loss: 1.3569\n",
            "Epoch [22/160], Training Loss: 1.2448, Training Accuracy: 78.63%\n",
            "Epoch [22/160], Validation Loss: 2.3579, Validation Accuracy: 43.53%\n",
            "Epoch [23/160], Step [16/32], Step Loss: 1.1346\n",
            "Epoch [23/160], Step [32/32], Step Loss: 0.9315\n",
            "Epoch [23/160], Training Loss: 1.1394, Training Accuracy: 81.67%\n",
            "Epoch [23/160], Validation Loss: 2.2311, Validation Accuracy: 44.90%\n",
            "Epoch [24/160], Step [16/32], Step Loss: 0.8948\n",
            "Epoch [24/160], Step [32/32], Step Loss: 1.1173\n",
            "Epoch [24/160], Training Loss: 1.0251, Training Accuracy: 83.63%\n",
            "Epoch [24/160], Validation Loss: 2.2017, Validation Accuracy: 46.08%\n",
            "Epoch [25/160], Step [16/32], Step Loss: 0.9470\n",
            "Epoch [25/160], Step [32/32], Step Loss: 0.7852\n",
            "Epoch [25/160], Training Loss: 0.9656, Training Accuracy: 86.08%\n",
            "Epoch [25/160], Validation Loss: 2.3623, Validation Accuracy: 42.25%\n",
            "Epoch [26/160], Step [16/32], Step Loss: 1.1256\n",
            "Epoch [26/160], Step [32/32], Step Loss: 0.8575\n",
            "Epoch [26/160], Training Loss: 0.9280, Training Accuracy: 86.96%\n",
            "Epoch [26/160], Validation Loss: 2.1580, Validation Accuracy: 46.67%\n",
            "Epoch [27/160], Step [16/32], Step Loss: 0.9280\n",
            "Epoch [27/160], Step [32/32], Step Loss: 0.7159\n",
            "Epoch [27/160], Training Loss: 0.8223, Training Accuracy: 89.61%\n",
            "Epoch [27/160], Validation Loss: 2.1361, Validation Accuracy: 46.67%\n",
            "Epoch [28/160], Step [16/32], Step Loss: 0.8801\n",
            "Epoch [28/160], Step [32/32], Step Loss: 0.6564\n",
            "Epoch [28/160], Training Loss: 0.7711, Training Accuracy: 91.08%\n",
            "Epoch [28/160], Validation Loss: 2.3424, Validation Accuracy: 43.73%\n",
            "Epoch [29/160], Step [16/32], Step Loss: 0.8362\n",
            "Epoch [29/160], Step [32/32], Step Loss: 0.6063\n",
            "Epoch [29/160], Training Loss: 0.7134, Training Accuracy: 90.88%\n",
            "Epoch [29/160], Validation Loss: 2.2002, Validation Accuracy: 45.00%\n",
            "Epoch [30/160], Step [16/32], Step Loss: 0.6864\n",
            "Epoch [30/160], Step [32/32], Step Loss: 0.6560\n",
            "Epoch [30/160], Training Loss: 0.6438, Training Accuracy: 93.43%\n",
            "Epoch [30/160], Validation Loss: 2.2600, Validation Accuracy: 43.24%\n",
            "Epoch [31/160], Step [16/32], Step Loss: 0.6445\n",
            "Epoch [31/160], Step [32/32], Step Loss: 0.5640\n",
            "Epoch [31/160], Training Loss: 0.5610, Training Accuracy: 94.51%\n",
            "Epoch [31/160], Validation Loss: 2.0639, Validation Accuracy: 49.90%\n",
            "Epoch [32/160], Step [16/32], Step Loss: 0.4901\n",
            "Epoch [32/160], Step [32/32], Step Loss: 0.6057\n",
            "Epoch [32/160], Training Loss: 0.5256, Training Accuracy: 95.29%\n",
            "Epoch [32/160], Validation Loss: 2.1085, Validation Accuracy: 45.20%\n",
            "Epoch [33/160], Step [16/32], Step Loss: 0.6274\n",
            "Epoch [33/160], Step [32/32], Step Loss: 0.5292\n",
            "Epoch [33/160], Training Loss: 0.5046, Training Accuracy: 94.71%\n",
            "Epoch [33/160], Validation Loss: 2.2402, Validation Accuracy: 45.98%\n",
            "Epoch [34/160], Step [16/32], Step Loss: 0.5407\n",
            "Epoch [34/160], Step [32/32], Step Loss: 0.5194\n",
            "Epoch [34/160], Training Loss: 0.4506, Training Accuracy: 96.27%\n",
            "Epoch [34/160], Validation Loss: 2.1224, Validation Accuracy: 47.45%\n",
            "Epoch [35/160], Step [16/32], Step Loss: 0.4182\n",
            "Epoch [35/160], Step [32/32], Step Loss: 0.3376\n",
            "Epoch [35/160], Training Loss: 0.3958, Training Accuracy: 96.96%\n",
            "Epoch [35/160], Validation Loss: 2.0626, Validation Accuracy: 48.43%\n",
            "Epoch [36/160], Step [16/32], Step Loss: 0.3282\n",
            "Epoch [36/160], Step [32/32], Step Loss: 0.5075\n",
            "Epoch [36/160], Training Loss: 0.3830, Training Accuracy: 97.16%\n",
            "Epoch [36/160], Validation Loss: 2.0891, Validation Accuracy: 49.80%\n",
            "Epoch [37/160], Step [16/32], Step Loss: 0.2852\n",
            "Epoch [37/160], Step [32/32], Step Loss: 0.2923\n",
            "Epoch [37/160], Training Loss: 0.3242, Training Accuracy: 98.24%\n",
            "Epoch [37/160], Validation Loss: 2.0354, Validation Accuracy: 49.61%\n",
            "Epoch [38/160], Step [16/32], Step Loss: 0.4043\n",
            "Epoch [38/160], Step [32/32], Step Loss: 0.4540\n",
            "Epoch [38/160], Training Loss: 0.3323, Training Accuracy: 97.75%\n",
            "Epoch [38/160], Validation Loss: 2.0620, Validation Accuracy: 49.80%\n",
            "Epoch [39/160], Step [16/32], Step Loss: 0.3635\n",
            "Epoch [39/160], Step [32/32], Step Loss: 0.2885\n",
            "Epoch [39/160], Training Loss: 0.2919, Training Accuracy: 98.73%\n",
            "Epoch [39/160], Validation Loss: 2.0084, Validation Accuracy: 50.69%\n",
            "Epoch [40/160], Step [16/32], Step Loss: 0.1625\n",
            "Epoch [40/160], Step [32/32], Step Loss: 0.4071\n",
            "Epoch [40/160], Training Loss: 0.2739, Training Accuracy: 98.73%\n",
            "Epoch [40/160], Validation Loss: 2.1457, Validation Accuracy: 47.16%\n",
            "Epoch [41/160], Step [16/32], Step Loss: 0.3416\n",
            "Epoch [41/160], Step [32/32], Step Loss: 0.3654\n",
            "Epoch [41/160], Training Loss: 0.2709, Training Accuracy: 98.73%\n",
            "Epoch [41/160], Validation Loss: 2.0050, Validation Accuracy: 51.86%\n",
            "Epoch [42/160], Step [16/32], Step Loss: 0.2268\n",
            "Epoch [42/160], Step [32/32], Step Loss: 0.3115\n",
            "Epoch [42/160], Training Loss: 0.2492, Training Accuracy: 98.73%\n",
            "Epoch [42/160], Validation Loss: 2.0554, Validation Accuracy: 49.41%\n",
            "Epoch [43/160], Step [16/32], Step Loss: 0.1869\n",
            "Epoch [43/160], Step [32/32], Step Loss: 0.3899\n",
            "Epoch [43/160], Training Loss: 0.2422, Training Accuracy: 98.92%\n",
            "Epoch [43/160], Validation Loss: 2.1274, Validation Accuracy: 46.96%\n",
            "Epoch [44/160], Step [16/32], Step Loss: 0.2179\n",
            "Epoch [44/160], Step [32/32], Step Loss: 0.2885\n",
            "Epoch [44/160], Training Loss: 0.2414, Training Accuracy: 99.41%\n",
            "Epoch [44/160], Validation Loss: 2.0773, Validation Accuracy: 49.71%\n",
            "Epoch [45/160], Step [16/32], Step Loss: 0.2521\n",
            "Epoch [45/160], Step [32/32], Step Loss: 0.2285\n",
            "Epoch [45/160], Training Loss: 0.2379, Training Accuracy: 99.12%\n",
            "Epoch [45/160], Validation Loss: 2.0736, Validation Accuracy: 49.71%\n",
            "Epoch [46/160], Step [16/32], Step Loss: 0.2029\n",
            "Epoch [46/160], Step [32/32], Step Loss: 0.2515\n",
            "Epoch [46/160], Training Loss: 0.2076, Training Accuracy: 99.51%\n",
            "Epoch [46/160], Validation Loss: 2.0525, Validation Accuracy: 48.92%\n",
            "Epoch [47/160], Step [16/32], Step Loss: 0.1636\n",
            "Epoch [47/160], Step [32/32], Step Loss: 0.2303\n",
            "Epoch [47/160], Training Loss: 0.2401, Training Accuracy: 98.73%\n",
            "Epoch [47/160], Validation Loss: 2.3017, Validation Accuracy: 44.51%\n",
            "Epoch [48/160], Step [16/32], Step Loss: 0.4971\n",
            "Epoch [48/160], Step [32/32], Step Loss: 0.3532\n",
            "Epoch [48/160], Training Loss: 0.2357, Training Accuracy: 98.92%\n",
            "Epoch [48/160], Validation Loss: 2.1594, Validation Accuracy: 48.82%\n",
            "Epoch [49/160], Step [16/32], Step Loss: 0.1609\n",
            "Epoch [49/160], Step [32/32], Step Loss: 0.2349\n",
            "Epoch [49/160], Training Loss: 0.1939, Training Accuracy: 99.41%\n",
            "Epoch [49/160], Validation Loss: 1.9987, Validation Accuracy: 49.71%\n",
            "Epoch [50/160], Step [16/32], Step Loss: 0.1261\n",
            "Epoch [50/160], Step [32/32], Step Loss: 0.1863\n",
            "Epoch [50/160], Training Loss: 0.1964, Training Accuracy: 99.61%\n",
            "Epoch [50/160], Validation Loss: 2.0851, Validation Accuracy: 49.71%\n",
            "Epoch [51/160], Step [16/32], Step Loss: 0.1808\n",
            "Epoch [51/160], Step [32/32], Step Loss: 0.2072\n",
            "Epoch [51/160], Training Loss: 0.1771, Training Accuracy: 99.31%\n",
            "Epoch [51/160], Validation Loss: 2.0914, Validation Accuracy: 47.75%\n",
            "Epoch [52/160], Step [16/32], Step Loss: 0.1802\n",
            "Epoch [52/160], Step [32/32], Step Loss: 0.1282\n",
            "Epoch [52/160], Training Loss: 0.1804, Training Accuracy: 99.51%\n",
            "Epoch [52/160], Validation Loss: 2.0018, Validation Accuracy: 50.98%\n",
            "Epoch [53/160], Step [16/32], Step Loss: 0.1658\n",
            "Epoch [53/160], Step [32/32], Step Loss: 0.3014\n",
            "Epoch [53/160], Training Loss: 0.1575, Training Accuracy: 99.41%\n",
            "Epoch [53/160], Validation Loss: 1.9808, Validation Accuracy: 51.27%\n",
            "Epoch [54/160], Step [16/32], Step Loss: 0.1595\n",
            "Epoch [54/160], Step [32/32], Step Loss: 0.1440\n",
            "Epoch [54/160], Training Loss: 0.1687, Training Accuracy: 99.22%\n",
            "Epoch [54/160], Validation Loss: 2.0008, Validation Accuracy: 51.37%\n",
            "Epoch [55/160], Step [16/32], Step Loss: 0.2043\n",
            "Epoch [55/160], Step [32/32], Step Loss: 0.1863\n",
            "Epoch [55/160], Training Loss: 0.1796, Training Accuracy: 99.61%\n",
            "Epoch [55/160], Validation Loss: 2.1059, Validation Accuracy: 47.55%\n",
            "Epoch [56/160], Step [16/32], Step Loss: 0.1447\n",
            "Epoch [56/160], Step [32/32], Step Loss: 0.2254\n",
            "Epoch [56/160], Training Loss: 0.1731, Training Accuracy: 99.61%\n",
            "Epoch [56/160], Validation Loss: 1.9872, Validation Accuracy: 50.88%\n",
            "Epoch [57/160], Step [16/32], Step Loss: 0.2099\n",
            "Epoch [57/160], Step [32/32], Step Loss: 0.2344\n",
            "Epoch [57/160], Training Loss: 0.1637, Training Accuracy: 99.22%\n",
            "Epoch [57/160], Validation Loss: 2.0778, Validation Accuracy: 48.24%\n",
            "Epoch [58/160], Step [16/32], Step Loss: 0.1173\n",
            "Epoch [58/160], Step [32/32], Step Loss: 0.1654\n",
            "Epoch [58/160], Training Loss: 0.1541, Training Accuracy: 99.90%\n",
            "Epoch [58/160], Validation Loss: 2.0057, Validation Accuracy: 51.86%\n",
            "Epoch [59/160], Step [16/32], Step Loss: 0.1233\n",
            "Epoch [59/160], Step [32/32], Step Loss: 0.1309\n",
            "Epoch [59/160], Training Loss: 0.1386, Training Accuracy: 100.00%\n",
            "Epoch [59/160], Validation Loss: 2.0390, Validation Accuracy: 49.51%\n",
            "Epoch [60/160], Step [16/32], Step Loss: 0.1124\n",
            "Epoch [60/160], Step [32/32], Step Loss: 0.0997\n",
            "Epoch [60/160], Training Loss: 0.1295, Training Accuracy: 99.80%\n",
            "Epoch [60/160], Validation Loss: 1.9999, Validation Accuracy: 51.76%\n",
            "Epoch [61/160], Step [16/32], Step Loss: 0.1581\n",
            "Epoch [61/160], Step [32/32], Step Loss: 0.2182\n",
            "Epoch [61/160], Training Loss: 0.1567, Training Accuracy: 99.61%\n",
            "Epoch [61/160], Validation Loss: 2.0198, Validation Accuracy: 48.73%\n",
            "Epoch [62/160], Step [16/32], Step Loss: 0.1326\n",
            "Epoch [62/160], Step [32/32], Step Loss: 0.1218\n",
            "Epoch [62/160], Training Loss: 0.1469, Training Accuracy: 99.80%\n",
            "Epoch [62/160], Validation Loss: 2.1004, Validation Accuracy: 49.80%\n",
            "Epoch [63/160], Step [16/32], Step Loss: 0.0898\n",
            "Epoch [63/160], Step [32/32], Step Loss: 0.2347\n",
            "Epoch [63/160], Training Loss: 0.1412, Training Accuracy: 100.00%\n",
            "Epoch [63/160], Validation Loss: 1.9477, Validation Accuracy: 53.33%\n",
            "Epoch [64/160], Step [16/32], Step Loss: 0.0989\n",
            "Epoch [64/160], Step [32/32], Step Loss: 0.1525\n",
            "Epoch [64/160], Training Loss: 0.1377, Training Accuracy: 99.80%\n",
            "Epoch [64/160], Validation Loss: 2.0558, Validation Accuracy: 50.88%\n",
            "Epoch [65/160], Step [16/32], Step Loss: 0.1046\n",
            "Epoch [65/160], Step [32/32], Step Loss: 0.1849\n",
            "Epoch [65/160], Training Loss: 0.1264, Training Accuracy: 100.00%\n",
            "Epoch [65/160], Validation Loss: 2.0839, Validation Accuracy: 48.53%\n",
            "Epoch [66/160], Step [16/32], Step Loss: 0.1304\n",
            "Epoch [66/160], Step [32/32], Step Loss: 0.1256\n",
            "Epoch [66/160], Training Loss: 0.1247, Training Accuracy: 99.90%\n",
            "Epoch [66/160], Validation Loss: 2.1163, Validation Accuracy: 47.45%\n",
            "Epoch [67/160], Step [16/32], Step Loss: 0.1343\n",
            "Epoch [67/160], Step [32/32], Step Loss: 0.1315\n",
            "Epoch [67/160], Training Loss: 0.1252, Training Accuracy: 100.00%\n",
            "Epoch [67/160], Validation Loss: 1.9672, Validation Accuracy: 52.55%\n",
            "Epoch [68/160], Step [16/32], Step Loss: 0.2225\n",
            "Epoch [68/160], Step [32/32], Step Loss: 0.1611\n",
            "Epoch [68/160], Training Loss: 0.1140, Training Accuracy: 99.90%\n",
            "Epoch [68/160], Validation Loss: 1.9843, Validation Accuracy: 50.98%\n",
            "Epoch [69/160], Step [16/32], Step Loss: 0.1230\n",
            "Epoch [69/160], Step [32/32], Step Loss: 0.1599\n",
            "Epoch [69/160], Training Loss: 0.1239, Training Accuracy: 100.00%\n",
            "Epoch [69/160], Validation Loss: 1.9518, Validation Accuracy: 51.57%\n",
            "Epoch [70/160], Step [16/32], Step Loss: 0.1444\n",
            "Epoch [70/160], Step [32/32], Step Loss: 0.1784\n",
            "Epoch [70/160], Training Loss: 0.1279, Training Accuracy: 100.00%\n",
            "Epoch [70/160], Validation Loss: 2.0505, Validation Accuracy: 51.27%\n",
            "Epoch [71/160], Step [16/32], Step Loss: 0.0930\n",
            "Epoch [71/160], Step [32/32], Step Loss: 0.1535\n",
            "Epoch [71/160], Training Loss: 0.1402, Training Accuracy: 99.90%\n",
            "Epoch [71/160], Validation Loss: 2.1168, Validation Accuracy: 49.51%\n",
            "Epoch [72/160], Step [16/32], Step Loss: 0.1223\n",
            "Epoch [72/160], Step [32/32], Step Loss: 0.1822\n",
            "Epoch [72/160], Training Loss: 0.1434, Training Accuracy: 99.80%\n",
            "Epoch [72/160], Validation Loss: 2.0983, Validation Accuracy: 47.94%\n",
            "Epoch [73/160], Step [16/32], Step Loss: 0.1874\n",
            "Epoch [73/160], Step [32/32], Step Loss: 0.1954\n",
            "Epoch [73/160], Training Loss: 0.1408, Training Accuracy: 99.80%\n",
            "Epoch [73/160], Validation Loss: 2.1008, Validation Accuracy: 48.53%\n",
            "Epoch [74/160], Step [16/32], Step Loss: 0.0987\n",
            "Epoch [74/160], Step [32/32], Step Loss: 0.1550\n",
            "Epoch [74/160], Training Loss: 0.1352, Training Accuracy: 100.00%\n",
            "Epoch [74/160], Validation Loss: 2.0183, Validation Accuracy: 51.27%\n",
            "Epoch 00074: reducing learning rate of group 0 to 5.0000e-06.\n",
            "Epoch [75/160], Step [16/32], Step Loss: 0.0896\n",
            "Epoch [75/160], Step [32/32], Step Loss: 0.0759\n",
            "Epoch [75/160], Training Loss: 0.0925, Training Accuracy: 100.00%\n",
            "Epoch [75/160], Validation Loss: 1.9018, Validation Accuracy: 53.43%\n",
            "Epoch [76/160], Step [16/32], Step Loss: 0.0716\n",
            "Epoch [76/160], Step [32/32], Step Loss: 0.0585\n",
            "Epoch [76/160], Training Loss: 0.0833, Training Accuracy: 99.80%\n",
            "Epoch [76/160], Validation Loss: 1.8802, Validation Accuracy: 54.12%\n",
            "Epoch [77/160], Step [16/32], Step Loss: 0.0681\n",
            "Epoch [77/160], Step [32/32], Step Loss: 0.0652\n",
            "Epoch [77/160], Training Loss: 0.0708, Training Accuracy: 100.00%\n",
            "Epoch [77/160], Validation Loss: 1.8858, Validation Accuracy: 53.82%\n",
            "Epoch [78/160], Step [16/32], Step Loss: 0.0628\n",
            "Epoch [78/160], Step [32/32], Step Loss: 0.0578\n",
            "Epoch [78/160], Training Loss: 0.0753, Training Accuracy: 100.00%\n",
            "Epoch [78/160], Validation Loss: 1.8794, Validation Accuracy: 53.73%\n",
            "Epoch [79/160], Step [16/32], Step Loss: 0.0732\n",
            "Epoch [79/160], Step [32/32], Step Loss: 0.0732\n",
            "Epoch [79/160], Training Loss: 0.0730, Training Accuracy: 99.90%\n",
            "Epoch [79/160], Validation Loss: 1.8752, Validation Accuracy: 53.73%\n",
            "Epoch [80/160], Step [16/32], Step Loss: 0.0550\n",
            "Epoch [80/160], Step [32/32], Step Loss: 0.0640\n",
            "Epoch [80/160], Training Loss: 0.0725, Training Accuracy: 99.80%\n",
            "Epoch [80/160], Validation Loss: 1.8814, Validation Accuracy: 53.73%\n",
            "Epoch [81/160], Step [16/32], Step Loss: 0.0867\n",
            "Epoch [81/160], Step [32/32], Step Loss: 0.0623\n",
            "Epoch [81/160], Training Loss: 0.0662, Training Accuracy: 100.00%\n",
            "Epoch [81/160], Validation Loss: 1.8643, Validation Accuracy: 53.82%\n",
            "Epoch [82/160], Step [16/32], Step Loss: 0.0664\n",
            "Epoch [82/160], Step [32/32], Step Loss: 0.0500\n",
            "Epoch [82/160], Training Loss: 0.0648, Training Accuracy: 100.00%\n",
            "Epoch [82/160], Validation Loss: 1.8850, Validation Accuracy: 53.82%\n",
            "Epoch [83/160], Step [16/32], Step Loss: 0.0604\n",
            "Epoch [83/160], Step [32/32], Step Loss: 0.0560\n",
            "Epoch [83/160], Training Loss: 0.0716, Training Accuracy: 100.00%\n",
            "Epoch [83/160], Validation Loss: 1.8892, Validation Accuracy: 54.31%\n",
            "Epoch [84/160], Step [16/32], Step Loss: 0.0521\n",
            "Epoch [84/160], Step [32/32], Step Loss: 0.0867\n",
            "Epoch [84/160], Training Loss: 0.0643, Training Accuracy: 100.00%\n",
            "Epoch [84/160], Validation Loss: 1.8662, Validation Accuracy: 53.43%\n",
            "Epoch [85/160], Step [16/32], Step Loss: 0.0685\n",
            "Epoch [85/160], Step [32/32], Step Loss: 0.0440\n",
            "Epoch [85/160], Training Loss: 0.0646, Training Accuracy: 100.00%\n",
            "Epoch [85/160], Validation Loss: 1.8617, Validation Accuracy: 54.02%\n",
            "Epoch [86/160], Step [16/32], Step Loss: 0.0471\n",
            "Epoch [86/160], Step [32/32], Step Loss: 0.0665\n",
            "Epoch [86/160], Training Loss: 0.0623, Training Accuracy: 100.00%\n",
            "Epoch [86/160], Validation Loss: 1.8669, Validation Accuracy: 53.92%\n",
            "Epoch [87/160], Step [16/32], Step Loss: 0.0686\n",
            "Epoch [87/160], Step [32/32], Step Loss: 0.0623\n",
            "Epoch [87/160], Training Loss: 0.0629, Training Accuracy: 100.00%\n",
            "Epoch [87/160], Validation Loss: 1.8792, Validation Accuracy: 54.02%\n",
            "Epoch [88/160], Step [16/32], Step Loss: 0.0769\n",
            "Epoch [88/160], Step [32/32], Step Loss: 0.0965\n",
            "Epoch [88/160], Training Loss: 0.0631, Training Accuracy: 99.90%\n",
            "Epoch [88/160], Validation Loss: 1.8696, Validation Accuracy: 53.73%\n",
            "Epoch [89/160], Step [16/32], Step Loss: 0.0831\n",
            "Epoch [89/160], Step [32/32], Step Loss: 0.0627\n",
            "Epoch [89/160], Training Loss: 0.0647, Training Accuracy: 100.00%\n",
            "Epoch [89/160], Validation Loss: 1.8554, Validation Accuracy: 54.41%\n",
            "Epoch [90/160], Step [16/32], Step Loss: 0.0676\n",
            "Epoch [90/160], Step [32/32], Step Loss: 0.0604\n",
            "Epoch [90/160], Training Loss: 0.0642, Training Accuracy: 100.00%\n",
            "Epoch [90/160], Validation Loss: 1.8589, Validation Accuracy: 54.61%\n",
            "Epoch [91/160], Step [16/32], Step Loss: 0.0629\n",
            "Epoch [91/160], Step [32/32], Step Loss: 0.0593\n",
            "Epoch [91/160], Training Loss: 0.0656, Training Accuracy: 100.00%\n",
            "Epoch [91/160], Validation Loss: 1.8620, Validation Accuracy: 55.00%\n",
            "Epoch [92/160], Step [16/32], Step Loss: 0.0947\n",
            "Epoch [92/160], Step [32/32], Step Loss: 0.0706\n",
            "Epoch [92/160], Training Loss: 0.0624, Training Accuracy: 100.00%\n",
            "Epoch [92/160], Validation Loss: 1.8612, Validation Accuracy: 54.22%\n",
            "Epoch [93/160], Step [16/32], Step Loss: 0.0682\n",
            "Epoch [93/160], Step [32/32], Step Loss: 0.0585\n",
            "Epoch [93/160], Training Loss: 0.0623, Training Accuracy: 100.00%\n",
            "Epoch [93/160], Validation Loss: 1.8660, Validation Accuracy: 53.82%\n",
            "Epoch [94/160], Step [16/32], Step Loss: 0.0731\n",
            "Epoch [94/160], Step [32/32], Step Loss: 0.0628\n",
            "Epoch [94/160], Training Loss: 0.0617, Training Accuracy: 100.00%\n",
            "Epoch [94/160], Validation Loss: 1.8559, Validation Accuracy: 54.41%\n",
            "Epoch [95/160], Step [16/32], Step Loss: 0.0524\n",
            "Epoch [95/160], Step [32/32], Step Loss: 0.0486\n",
            "Epoch [95/160], Training Loss: 0.0675, Training Accuracy: 100.00%\n",
            "Epoch [95/160], Validation Loss: 1.8515, Validation Accuracy: 53.82%\n",
            "Epoch [96/160], Step [16/32], Step Loss: 0.0586\n",
            "Epoch [96/160], Step [32/32], Step Loss: 0.0670\n",
            "Epoch [96/160], Training Loss: 0.0648, Training Accuracy: 100.00%\n",
            "Epoch [96/160], Validation Loss: 1.8670, Validation Accuracy: 54.41%\n",
            "Epoch [97/160], Step [16/32], Step Loss: 0.0651\n",
            "Epoch [97/160], Step [32/32], Step Loss: 0.0531\n",
            "Epoch [97/160], Training Loss: 0.0627, Training Accuracy: 100.00%\n",
            "Epoch [97/160], Validation Loss: 1.8555, Validation Accuracy: 54.02%\n",
            "Epoch [98/160], Step [16/32], Step Loss: 0.0555\n",
            "Epoch [98/160], Step [32/32], Step Loss: 0.0609\n",
            "Epoch [98/160], Training Loss: 0.0654, Training Accuracy: 100.00%\n",
            "Epoch [98/160], Validation Loss: 1.8561, Validation Accuracy: 53.92%\n",
            "Epoch [99/160], Step [16/32], Step Loss: 0.0478\n",
            "Epoch [99/160], Step [32/32], Step Loss: 0.0723\n",
            "Epoch [99/160], Training Loss: 0.0657, Training Accuracy: 100.00%\n",
            "Epoch [99/160], Validation Loss: 1.8579, Validation Accuracy: 53.92%\n",
            "Epoch [100/160], Step [16/32], Step Loss: 0.0666\n",
            "Epoch [100/160], Step [32/32], Step Loss: 0.0628\n",
            "Epoch [100/160], Training Loss: 0.0662, Training Accuracy: 100.00%\n",
            "Epoch [100/160], Validation Loss: 1.8564, Validation Accuracy: 54.41%\n",
            "Epoch [101/160], Step [16/32], Step Loss: 0.0548\n",
            "Epoch [101/160], Step [32/32], Step Loss: 0.0630\n",
            "Epoch [101/160], Training Loss: 0.0665, Training Accuracy: 100.00%\n",
            "Epoch [101/160], Validation Loss: 1.8695, Validation Accuracy: 53.82%\n",
            "Epoch [102/160], Step [16/32], Step Loss: 0.0854\n",
            "Epoch [102/160], Step [32/32], Step Loss: 0.0596\n",
            "Epoch [102/160], Training Loss: 0.0647, Training Accuracy: 100.00%\n",
            "Epoch [102/160], Validation Loss: 1.8597, Validation Accuracy: 54.41%\n",
            "Epoch [103/160], Step [16/32], Step Loss: 0.0807\n",
            "Epoch [103/160], Step [32/32], Step Loss: 0.0671\n",
            "Epoch [103/160], Training Loss: 0.0685, Training Accuracy: 100.00%\n",
            "Epoch [103/160], Validation Loss: 1.8643, Validation Accuracy: 53.82%\n",
            "Epoch [104/160], Step [16/32], Step Loss: 0.0660\n",
            "Epoch [104/160], Step [32/32], Step Loss: 0.0655\n",
            "Epoch [104/160], Training Loss: 0.0637, Training Accuracy: 100.00%\n",
            "Epoch [104/160], Validation Loss: 1.8744, Validation Accuracy: 54.02%\n",
            "Epoch [105/160], Step [16/32], Step Loss: 0.0863\n",
            "Epoch [105/160], Step [32/32], Step Loss: 0.0692\n",
            "Epoch [105/160], Training Loss: 0.0695, Training Accuracy: 100.00%\n",
            "Epoch [105/160], Validation Loss: 1.8545, Validation Accuracy: 54.22%\n",
            "Epoch [106/160], Step [16/32], Step Loss: 0.0784\n",
            "Epoch [106/160], Step [32/32], Step Loss: 0.0615\n",
            "Epoch [106/160], Training Loss: 0.0696, Training Accuracy: 100.00%\n",
            "Epoch [106/160], Validation Loss: 1.8654, Validation Accuracy: 53.92%\n",
            "Epoch 00106: reducing learning rate of group 0 to 5.0000e-07.\n",
            "Epoch [107/160], Step [16/32], Step Loss: 0.0486\n",
            "Epoch [107/160], Step [32/32], Step Loss: 0.0659\n",
            "Epoch [107/160], Training Loss: 0.0653, Training Accuracy: 100.00%\n",
            "Epoch [107/160], Validation Loss: 1.8674, Validation Accuracy: 53.73%\n",
            "Epoch [108/160], Step [16/32], Step Loss: 0.0655\n",
            "Epoch [108/160], Step [32/32], Step Loss: 0.0453\n",
            "Epoch [108/160], Training Loss: 0.0650, Training Accuracy: 100.00%\n",
            "Epoch [108/160], Validation Loss: 1.8703, Validation Accuracy: 53.43%\n",
            "Epoch [109/160], Step [16/32], Step Loss: 0.0801\n",
            "Epoch [109/160], Step [32/32], Step Loss: 0.0550\n",
            "Epoch [109/160], Training Loss: 0.0634, Training Accuracy: 100.00%\n",
            "Epoch [109/160], Validation Loss: 1.8658, Validation Accuracy: 53.82%\n",
            "Epoch [110/160], Step [16/32], Step Loss: 0.0582\n",
            "Epoch [110/160], Step [32/32], Step Loss: 0.0857\n",
            "Epoch [110/160], Training Loss: 0.0645, Training Accuracy: 100.00%\n",
            "Epoch [110/160], Validation Loss: 1.8550, Validation Accuracy: 53.63%\n",
            "Epoch [111/160], Step [16/32], Step Loss: 0.0552\n",
            "Epoch [111/160], Step [32/32], Step Loss: 0.0514\n",
            "Epoch [111/160], Training Loss: 0.0625, Training Accuracy: 100.00%\n",
            "Epoch [111/160], Validation Loss: 1.8550, Validation Accuracy: 54.41%\n",
            "Epoch [112/160], Step [16/32], Step Loss: 0.0663\n",
            "Epoch [112/160], Step [32/32], Step Loss: 0.0732\n",
            "Epoch [112/160], Training Loss: 0.0616, Training Accuracy: 100.00%\n",
            "Epoch [112/160], Validation Loss: 1.8529, Validation Accuracy: 53.73%\n",
            "Epoch [113/160], Step [16/32], Step Loss: 0.0568\n",
            "Epoch [113/160], Step [32/32], Step Loss: 0.0766\n",
            "Epoch [113/160], Training Loss: 0.0658, Training Accuracy: 100.00%\n",
            "Epoch [113/160], Validation Loss: 1.8616, Validation Accuracy: 53.73%\n",
            "Epoch [114/160], Step [16/32], Step Loss: 0.0559\n",
            "Epoch [114/160], Step [32/32], Step Loss: 0.0643\n",
            "Epoch [114/160], Training Loss: 0.0634, Training Accuracy: 100.00%\n",
            "Epoch [114/160], Validation Loss: 1.8564, Validation Accuracy: 54.31%\n",
            "Epoch [115/160], Step [16/32], Step Loss: 0.0739\n",
            "Epoch [115/160], Step [32/32], Step Loss: 0.0681\n",
            "Epoch [115/160], Training Loss: 0.0646, Training Accuracy: 100.00%\n",
            "Epoch [115/160], Validation Loss: 1.8589, Validation Accuracy: 53.53%\n",
            "Epoch [116/160], Step [16/32], Step Loss: 0.0550\n",
            "Epoch [116/160], Step [32/32], Step Loss: 0.0802\n",
            "Epoch [116/160], Training Loss: 0.0665, Training Accuracy: 100.00%\n",
            "Epoch [116/160], Validation Loss: 1.8552, Validation Accuracy: 54.02%\n",
            "Epoch [117/160], Step [16/32], Step Loss: 0.0555\n",
            "Epoch [117/160], Step [32/32], Step Loss: 0.1039\n",
            "Epoch [117/160], Training Loss: 0.0641, Training Accuracy: 100.00%\n",
            "Epoch [117/160], Validation Loss: 1.8685, Validation Accuracy: 54.22%\n",
            "Epoch 00117: reducing learning rate of group 0 to 5.0000e-08.\n",
            "Epoch [118/160], Step [16/32], Step Loss: 0.0542\n",
            "Epoch [118/160], Step [32/32], Step Loss: 0.0688\n",
            "Epoch [118/160], Training Loss: 0.0663, Training Accuracy: 100.00%\n",
            "Epoch [118/160], Validation Loss: 1.8543, Validation Accuracy: 54.31%\n",
            "Epoch [119/160], Step [16/32], Step Loss: 0.0584\n",
            "Epoch [119/160], Step [32/32], Step Loss: 0.0828\n",
            "Epoch [119/160], Training Loss: 0.0620, Training Accuracy: 100.00%\n",
            "Epoch [119/160], Validation Loss: 1.8604, Validation Accuracy: 54.22%\n",
            "Epoch [120/160], Step [16/32], Step Loss: 0.0759\n",
            "Epoch [120/160], Step [32/32], Step Loss: 0.0591\n",
            "Epoch [120/160], Training Loss: 0.0610, Training Accuracy: 100.00%\n",
            "Epoch [120/160], Validation Loss: 1.8675, Validation Accuracy: 54.22%\n",
            "Epoch [121/160], Step [16/32], Step Loss: 0.0631\n",
            "Epoch [121/160], Step [32/32], Step Loss: 0.0662\n",
            "Epoch [121/160], Training Loss: 0.0637, Training Accuracy: 100.00%\n",
            "Epoch [121/160], Validation Loss: 1.8612, Validation Accuracy: 54.41%\n",
            "Epoch [122/160], Step [16/32], Step Loss: 0.0752\n",
            "Epoch [122/160], Step [32/32], Step Loss: 0.0632\n",
            "Epoch [122/160], Training Loss: 0.0688, Training Accuracy: 100.00%\n",
            "Epoch [122/160], Validation Loss: 1.8594, Validation Accuracy: 53.63%\n",
            "Epoch [123/160], Step [16/32], Step Loss: 0.0676\n",
            "Epoch [123/160], Step [32/32], Step Loss: 0.0869\n",
            "Epoch [123/160], Training Loss: 0.0644, Training Accuracy: 100.00%\n",
            "Epoch [123/160], Validation Loss: 1.8492, Validation Accuracy: 53.82%\n",
            "Epoch [124/160], Step [16/32], Step Loss: 0.0590\n",
            "Epoch [124/160], Step [32/32], Step Loss: 0.0598\n",
            "Epoch [124/160], Training Loss: 0.0651, Training Accuracy: 100.00%\n",
            "Epoch [124/160], Validation Loss: 1.8573, Validation Accuracy: 54.41%\n",
            "Epoch [125/160], Step [16/32], Step Loss: 0.0815\n",
            "Epoch [125/160], Step [32/32], Step Loss: 0.0563\n",
            "Epoch [125/160], Training Loss: 0.0618, Training Accuracy: 100.00%\n",
            "Epoch [125/160], Validation Loss: 1.8577, Validation Accuracy: 54.61%\n",
            "Epoch [126/160], Step [16/32], Step Loss: 0.0512\n",
            "Epoch [126/160], Step [32/32], Step Loss: 0.0737\n",
            "Epoch [126/160], Training Loss: 0.0640, Training Accuracy: 100.00%\n",
            "Epoch [126/160], Validation Loss: 1.8585, Validation Accuracy: 54.41%\n",
            "Epoch [127/160], Step [16/32], Step Loss: 0.0623\n",
            "Epoch [127/160], Step [32/32], Step Loss: 0.0475\n",
            "Epoch [127/160], Training Loss: 0.0644, Training Accuracy: 100.00%\n",
            "Epoch [127/160], Validation Loss: 1.8543, Validation Accuracy: 54.22%\n",
            "Epoch [128/160], Step [16/32], Step Loss: 0.1069\n",
            "Epoch [128/160], Step [32/32], Step Loss: 0.0503\n",
            "Epoch [128/160], Training Loss: 0.0650, Training Accuracy: 100.00%\n",
            "Epoch [128/160], Validation Loss: 1.8684, Validation Accuracy: 54.71%\n",
            "Epoch [129/160], Step [16/32], Step Loss: 0.0595\n",
            "Epoch [129/160], Step [32/32], Step Loss: 0.0572\n",
            "Epoch [129/160], Training Loss: 0.0648, Training Accuracy: 100.00%\n",
            "Epoch [129/160], Validation Loss: 1.8516, Validation Accuracy: 53.63%\n",
            "Epoch [130/160], Step [16/32], Step Loss: 0.0664\n",
            "Epoch [130/160], Step [32/32], Step Loss: 0.0860\n",
            "Epoch [130/160], Training Loss: 0.0647, Training Accuracy: 100.00%\n",
            "Epoch [130/160], Validation Loss: 1.8595, Validation Accuracy: 54.31%\n",
            "Epoch [131/160], Step [16/32], Step Loss: 0.0570\n",
            "Epoch [131/160], Step [32/32], Step Loss: 0.1111\n",
            "Epoch [131/160], Training Loss: 0.0680, Training Accuracy: 100.00%\n",
            "Epoch [131/160], Validation Loss: 1.8618, Validation Accuracy: 53.73%\n",
            "Epoch [132/160], Step [16/32], Step Loss: 0.0567\n",
            "Epoch [132/160], Step [32/32], Step Loss: 0.0589\n",
            "Epoch [132/160], Training Loss: 0.0673, Training Accuracy: 100.00%\n",
            "Epoch [132/160], Validation Loss: 1.8648, Validation Accuracy: 53.92%\n",
            "Epoch [133/160], Step [16/32], Step Loss: 0.0535\n",
            "Epoch [133/160], Step [32/32], Step Loss: 0.0669\n",
            "Epoch [133/160], Training Loss: 0.0665, Training Accuracy: 100.00%\n",
            "Epoch [133/160], Validation Loss: 1.8553, Validation Accuracy: 54.31%\n",
            "Epoch [134/160], Step [16/32], Step Loss: 0.0646\n",
            "Epoch [134/160], Step [32/32], Step Loss: 0.0767\n",
            "Epoch [134/160], Training Loss: 0.0610, Training Accuracy: 100.00%\n",
            "Epoch [134/160], Validation Loss: 1.8542, Validation Accuracy: 54.22%\n",
            "Epoch 00134: reducing learning rate of group 0 to 5.0000e-09.\n",
            "Epoch [135/160], Step [16/32], Step Loss: 0.0733\n",
            "Epoch [135/160], Step [32/32], Step Loss: 0.1336\n",
            "Epoch [135/160], Training Loss: 0.0643, Training Accuracy: 100.00%\n",
            "Epoch [135/160], Validation Loss: 1.8566, Validation Accuracy: 54.51%\n",
            "Epoch [136/160], Step [16/32], Step Loss: 0.0598\n",
            "Epoch [136/160], Step [32/32], Step Loss: 0.0712\n",
            "Epoch [136/160], Training Loss: 0.0627, Training Accuracy: 100.00%\n",
            "Epoch [136/160], Validation Loss: 1.8572, Validation Accuracy: 54.71%\n",
            "Epoch [137/160], Step [16/32], Step Loss: 0.0957\n",
            "Epoch [137/160], Step [32/32], Step Loss: 0.0585\n",
            "Epoch [137/160], Training Loss: 0.0631, Training Accuracy: 100.00%\n",
            "Epoch [137/160], Validation Loss: 1.8569, Validation Accuracy: 53.92%\n",
            "Epoch [138/160], Step [16/32], Step Loss: 0.0709\n",
            "Epoch [138/160], Step [32/32], Step Loss: 0.0646\n",
            "Epoch [138/160], Training Loss: 0.0617, Training Accuracy: 100.00%\n",
            "Epoch [138/160], Validation Loss: 1.8562, Validation Accuracy: 54.02%\n",
            "Epoch [139/160], Step [16/32], Step Loss: 0.0688\n",
            "Epoch [139/160], Step [32/32], Step Loss: 0.0723\n",
            "Epoch [139/160], Training Loss: 0.0656, Training Accuracy: 100.00%\n",
            "Epoch [139/160], Validation Loss: 1.8601, Validation Accuracy: 54.80%\n",
            "Epoch [140/160], Step [16/32], Step Loss: 0.0619\n",
            "Epoch [140/160], Step [32/32], Step Loss: 0.0630\n",
            "Epoch [140/160], Training Loss: 0.0642, Training Accuracy: 99.90%\n",
            "Epoch [140/160], Validation Loss: 1.8537, Validation Accuracy: 53.73%\n",
            "Epoch [141/160], Step [16/32], Step Loss: 0.0705\n",
            "Epoch [141/160], Step [32/32], Step Loss: 0.1113\n",
            "Epoch [141/160], Training Loss: 0.0686, Training Accuracy: 100.00%\n",
            "Epoch [141/160], Validation Loss: 1.8652, Validation Accuracy: 54.41%\n",
            "Epoch [142/160], Step [16/32], Step Loss: 0.0806\n",
            "Epoch [142/160], Step [32/32], Step Loss: 0.0747\n",
            "Epoch [142/160], Training Loss: 0.0624, Training Accuracy: 100.00%\n",
            "Epoch [142/160], Validation Loss: 1.8626, Validation Accuracy: 55.29%\n",
            "Epoch [143/160], Step [16/32], Step Loss: 0.0718\n",
            "Epoch [143/160], Step [32/32], Step Loss: 0.0560\n",
            "Epoch [143/160], Training Loss: 0.0635, Training Accuracy: 100.00%\n",
            "Epoch [143/160], Validation Loss: 1.8561, Validation Accuracy: 54.12%\n",
            "Epoch [144/160], Step [16/32], Step Loss: 0.0876\n",
            "Epoch [144/160], Step [32/32], Step Loss: 0.0646\n",
            "Epoch [144/160], Training Loss: 0.0635, Training Accuracy: 100.00%\n",
            "Epoch [144/160], Validation Loss: 1.8566, Validation Accuracy: 54.22%\n",
            "Epoch [145/160], Step [16/32], Step Loss: 0.0624\n",
            "Epoch [145/160], Step [32/32], Step Loss: 0.0589\n",
            "Epoch [145/160], Training Loss: 0.0685, Training Accuracy: 100.00%\n",
            "Epoch [145/160], Validation Loss: 1.8554, Validation Accuracy: 53.82%\n",
            "Epoch [146/160], Step [16/32], Step Loss: 0.0493\n",
            "Epoch [146/160], Step [32/32], Step Loss: 0.0595\n",
            "Epoch [146/160], Training Loss: 0.0694, Training Accuracy: 100.00%\n",
            "Epoch [146/160], Validation Loss: 1.8675, Validation Accuracy: 54.31%\n",
            "Epoch [147/160], Step [16/32], Step Loss: 0.0682\n",
            "Epoch [147/160], Step [32/32], Step Loss: 0.0962\n",
            "Epoch [147/160], Training Loss: 0.0647, Training Accuracy: 100.00%\n",
            "Epoch [147/160], Validation Loss: 1.8602, Validation Accuracy: 54.31%\n",
            "Epoch [148/160], Step [16/32], Step Loss: 0.1090\n",
            "Epoch [148/160], Step [32/32], Step Loss: 0.0578\n",
            "Epoch [148/160], Training Loss: 0.0629, Training Accuracy: 100.00%\n",
            "Epoch [148/160], Validation Loss: 1.8546, Validation Accuracy: 53.82%\n",
            "Epoch [149/160], Step [16/32], Step Loss: 0.0544\n",
            "Epoch [149/160], Step [32/32], Step Loss: 0.0674\n",
            "Epoch [149/160], Training Loss: 0.0637, Training Accuracy: 100.00%\n",
            "Epoch [149/160], Validation Loss: 1.8533, Validation Accuracy: 53.73%\n",
            "Epoch [150/160], Step [16/32], Step Loss: 0.0480\n",
            "Epoch [150/160], Step [32/32], Step Loss: 0.0825\n",
            "Epoch [150/160], Training Loss: 0.0634, Training Accuracy: 100.00%\n",
            "Epoch [150/160], Validation Loss: 1.8528, Validation Accuracy: 54.02%\n",
            "Epoch [151/160], Step [16/32], Step Loss: 0.0632\n",
            "Epoch [151/160], Step [32/32], Step Loss: 0.0529\n",
            "Epoch [151/160], Training Loss: 0.0618, Training Accuracy: 100.00%\n",
            "Epoch [151/160], Validation Loss: 1.8594, Validation Accuracy: 54.12%\n",
            "Epoch [152/160], Step [16/32], Step Loss: 0.0789\n",
            "Epoch [152/160], Step [32/32], Step Loss: 0.0564\n",
            "Epoch [152/160], Training Loss: 0.0650, Training Accuracy: 100.00%\n",
            "Epoch [152/160], Validation Loss: 1.8531, Validation Accuracy: 54.51%\n",
            "Epoch [153/160], Step [16/32], Step Loss: 0.0508\n",
            "Epoch [153/160], Step [32/32], Step Loss: 0.0654\n",
            "Epoch [153/160], Training Loss: 0.0618, Training Accuracy: 100.00%\n",
            "Epoch [153/160], Validation Loss: 1.8689, Validation Accuracy: 53.92%\n",
            "Epoch [154/160], Step [16/32], Step Loss: 0.0722\n",
            "Epoch [154/160], Step [32/32], Step Loss: 0.0629\n",
            "Epoch [154/160], Training Loss: 0.0653, Training Accuracy: 100.00%\n",
            "Epoch [154/160], Validation Loss: 1.8575, Validation Accuracy: 53.82%\n",
            "Epoch [155/160], Step [16/32], Step Loss: 0.0661\n",
            "Epoch [155/160], Step [32/32], Step Loss: 0.0706\n",
            "Epoch [155/160], Training Loss: 0.0624, Training Accuracy: 100.00%\n",
            "Epoch [155/160], Validation Loss: 1.8546, Validation Accuracy: 55.00%\n",
            "Epoch [156/160], Step [16/32], Step Loss: 0.0683\n",
            "Epoch [156/160], Step [32/32], Step Loss: 0.0685\n",
            "Epoch [156/160], Training Loss: 0.0654, Training Accuracy: 100.00%\n",
            "Epoch [156/160], Validation Loss: 1.8550, Validation Accuracy: 53.92%\n",
            "Epoch [157/160], Step [16/32], Step Loss: 0.0538\n",
            "Epoch [157/160], Step [32/32], Step Loss: 0.0542\n",
            "Epoch [157/160], Training Loss: 0.0618, Training Accuracy: 100.00%\n",
            "Epoch [157/160], Validation Loss: 1.8625, Validation Accuracy: 53.73%\n",
            "Epoch [158/160], Step [16/32], Step Loss: 0.0431\n",
            "Epoch [158/160], Step [32/32], Step Loss: 0.0613\n",
            "Epoch [158/160], Training Loss: 0.0625, Training Accuracy: 100.00%\n",
            "Epoch [158/160], Validation Loss: 1.8578, Validation Accuracy: 54.22%\n",
            "Epoch [159/160], Step [16/32], Step Loss: 0.0666\n",
            "Epoch [159/160], Step [32/32], Step Loss: 0.0593\n",
            "Epoch [159/160], Training Loss: 0.0698, Training Accuracy: 100.00%\n",
            "Epoch [159/160], Validation Loss: 1.8523, Validation Accuracy: 54.41%\n",
            "Epoch [160/160], Step [16/32], Step Loss: 0.0534\n",
            "Epoch [160/160], Step [32/32], Step Loss: 0.0600\n",
            "Epoch [160/160], Training Loss: 0.0623, Training Accuracy: 100.00%\n",
            "Epoch [160/160], Validation Loss: 1.8565, Validation Accuracy: 53.92%\n",
            "Test Accuracy: 50.35%\n"
          ]
        }
      ],
      "source": [
        "# Train the CNN\n",
        "for epoch in range(EPOCHS):\n",
        "    cnn.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = cnn(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        if (i + 1) % 16 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{EPOCHS}], Step [{i + 1}/{len(train_loader)}], Step Loss: {loss.item():.4f}\")\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = 100 * train_correct / train_total\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Evaluate model after each training epoch\n",
        "    cnn.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = cnn(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch [{epoch+1}/{EPOCHS}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "    # Update the learning rate scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "# Test the CNN on the test set\n",
        "cnn.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = cnn(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n"
      ]
    }
  ]
}