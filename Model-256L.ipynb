{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1682812403121,
     "user": {
      "displayName": "Faran Lane",
      "userId": "14277689357306891726"
     },
     "user_tz": -60
    },
    "id": "lj0C9_xsyNeN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Notes\n",
    "# Batch size 32-128\n",
    "# lr <= 0.0001 for relu\n",
    "# if we want more data so training > test, can use data augmentation\n",
    "# experimented with some dropout layers to prevent overfitting, low probability on conv layers, 0.4+ on fc layers\n",
    "# relu, CrossEntropy and Adam seem to make most sense"
   ],
   "id": "lj0C9_xsyNeN"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:4572k4yd) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">scruffy-looking-transport-1</strong> at: <a href='https://wandb.ai/faran-team/testproj/runs/4572k4yd' target=\"_blank\">https://wandb.ai/faran-team/testproj/runs/4572k4yd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>.\\wandb\\run-20230504_185910-4572k4yd\\logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:4572k4yd). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3deb120065f14bbdb5a1e005649961f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.15.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\lanef\\uni\\Code\\Int2Github\\wandb\\run-20230504_190355-m7kkrthi</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/faran-team/testproj/runs/m7kkrthi' target=\"_blank\">galactic-fleet-2</a></strong> to <a href='https://wandb.ai/faran-team/testproj' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/faran-team/testproj' target=\"_blank\">https://wandb.ai/faran-team/testproj</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/faran-team/testproj/runs/m7kkrthi' target=\"_blank\">https://wandb.ai/faran-team/testproj/runs/m7kkrthi</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/faran-team/testproj/runs/m7kkrthi?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x2099225bb50>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46473,
     "status": "ok",
     "timestamp": 1682812450886,
     "user": {
      "displayName": "Faran Lane",
      "userId": "14277689357306891726"
     },
     "user_tz": -60
    },
    "id": "EusKGDDAyNeR",
    "outputId": "0796883d-22e0-4578-a96f-facc60d7d9cf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch, time, gc\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grid\n",
    "\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {torch.cuda.get_device_name(0)}')\n",
    "SAVE_PATH = \"./saves/neural_net.pth\"\n",
    "\n",
    "# Training variables\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.00001\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "# Define the transforms for the dataset\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(256, antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "train_transform = transforms.Compose([\n",
    "    base_transform,\n",
    "    transforms.CenterCrop(256)\n",
    "])\n",
    "augment_transform = transforms.Compose([\n",
    "    base_transform,\n",
    "    transforms.RandomApply(\n",
    "        [transforms.RandomApply([transforms.RandomAffine(degrees=359, translate=(0.2, 0.2), shear=(20, 20, 20, 20)),\n",
    "                                 transforms.RandomHorizontalFlip(1), transforms.RandomVerticalFlip(1)], p=0.7),\n",
    "         transforms.ColorJitter(brightness=(0.75, 1.25), contrast=(0.75, 1.25), hue=(-0.15, 0.15),\n",
    "                                saturation=(0.75, 1.25))\n",
    "         ]\n",
    "    )\n",
    "])\n",
    "\n",
    "val_accuracies, val_losses, train_accuracies, train_losses = [], [], [], []\n",
    "\n",
    "# Load the train dataset\n",
    "train_dataset = datasets.Flowers102(root=\"./data\", split=\"train\", transform=base_transform, download=True)\n",
    "# train_miniset = Subset(train_dataset, list(random.randint(0, len(train_dataset)) for i in range(32)))\n",
    "all_train_data = [train_dataset]\n",
    "\n",
    "# Since augment transform has probability/random changes, we can use it multiple times to generate more training data\n",
    "for i in range(1):\n",
    "    all_train_data.append(\n",
    "        datasets.Flowers102(root=\"./data\", split=\"train\", transform=augment_transform, download=False))\n",
    "train_dataset_extra = torch.utils.data.ConcatDataset(all_train_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset_extra, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = datasets.Flowers102(root=\"./data\", split=\"val\", transform=base_transform, download=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "test_dataset = datasets.Flowers102(root=\"./data\", split=\"test\", transform=base_transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "                                   nn.BatchNorm2d(64),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2, 2))  # 256 -> 128\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "                                   nn.BatchNorm2d(128),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2, 2)) # 128 -> 64\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "                                   nn.BatchNorm2d(128),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2, 2)) # 64 -> 32\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2, 2))  # 32 -> 16\n",
    "        self.conv5 = nn.Sequential(nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "                                   nn.BatchNorm2d(256),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2, 2))  # 16 -> 8, 8 -> 4, 4 -> 2\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Sequential(nn.Linear(2 * 2 * 256, 1024),\n",
    "                                 nn.ReLU())\n",
    "        # nn.Dropout(0.65))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(1024, 1024),\n",
    "                                 nn.ReLU())\n",
    "        # nn.Dropout(0.75))\n",
    "        self.fc3 = nn.Linear(1024, 102)\n",
    "        # Final output_features size must be 102 as there are 102 labels in the dataset\n",
    "        # Pool dimensions: out=(in-kernel)/stride + 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        # Final output_features size must be 102 as there are 102 labels in the dataset\n",
    "        # Pool dimensions: out=(in-kernel)/stride + 1"
   ],
   "id": "EusKGDDAyNeR"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def train_one_epoch(net: nn.Module = None, training_loss: [] = None, training_acc: [] = None, optimizer=None):\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.AdamW(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        labels = torch.eye(102)[labels]  # one hot encode\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = net(images)  # train\n",
    "        labels = torch.argmax(labels, dim=1)  # one hot decode\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += torch.sum(predicted == labels).sum().item()\n",
    "\n",
    "    running_loss /= len(all_train_data)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    # save data\n",
    "    training_loss.append(running_loss)\n",
    "    training_acc.append(train_accuracy)\n",
    "    print(f'Training Loss: {running_loss:.4f}, Accuracy: {train_accuracy:.2f}%')\n",
    "    return training_loss, training_acc\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def validate_one_epoch(net: nn.Module = None, valid_loss: [] = None, valid_acc: [] = None):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        running_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        vtotal = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            vloss = criterion(outputs, labels)\n",
    "            running_vloss += vloss.item() * images.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            vtotal += labels.size(0)\n",
    "            vcorrect += (predicted == labels).sum().item()\n",
    "\n",
    "        vaccuracy = 100 * vcorrect / vtotal\n",
    "\n",
    "        # save validation data\n",
    "        running_vloss /= len(val_dataset)\n",
    "        valid_loss.append(running_vloss)\n",
    "        valid_acc.append(vaccuracy)\n",
    "        print(f'Validation Loss: {running_vloss:.4f}, Accuracy: {vaccuracy:.2f}%')\n",
    "        return valid_loss, valid_acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def train_net(net: nn.Module = None, epochs: int = 1, validation_loss=None, validation_accuracy=None,\n",
    "              training_loss=None, training_accuracy=None, epochs_already_trained: int = 0):\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if training_accuracy is None:\n",
    "        training_accuracy = []\n",
    "    if training_loss is None:\n",
    "        training_loss = []\n",
    "    if validation_accuracy is None:\n",
    "        validation_accuracy = []\n",
    "    if validation_loss is None:\n",
    "        validation_loss = []\n",
    "\n",
    "    epochs += epochs_already_trained\n",
    "    for epoch in range(epochs_already_trained, epochs):\n",
    "        print(f'Epoch: [{epoch + 1}/{epochs}]')\n",
    "\n",
    "        training_loss, training_accuracy = train_one_epoch(net, training_loss=training_loss,\n",
    "                                                           training_acc=training_accuracy, optimizer=optimizer)\n",
    "        validation_loss, validation_accuracy = validate_one_epoch(net, valid_loss=validation_loss,\n",
    "                                                                  valid_acc=validation_accuracy)\n",
    "\n",
    "        torch.save(net.state_dict(), SAVE_PATH)\n",
    "        with open(\"./saves/save_data.txt\", \"w\") as savefile:\n",
    "            savefile.writelines(str(validation_loss))\n",
    "            savefile.writelines(str(validation_accuracy))\n",
    "            savefile.writelines(str(training_loss))\n",
    "            savefile.writelines(str(training_accuracy))\n",
    "            savefile.write(str(epoch + 1))\n",
    "\n",
    "        wandb.log({\"train_acc\": training_accuracy, \"train_loss\": training_loss, \"val_acc\": validation_accuracy, \"val_loss\": validation_loss})\n",
    "        print(\"NN saved.\")\n",
    "    wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def train_net_from_save(epochs: int = 1, save_folder_path: str = \"./saves\"):\n",
    "    print(\"Resuming training from save.\")\n",
    "    net = CNN().to(device)\n",
    "    net.load_state_dict(torch.load(save_folder_path + \"/neural_net.pth\"))\n",
    "    with open(save_folder_path + \"/save_data.txt\", \"r\") as savefile:\n",
    "        savelist = savefile.readline().split(\"]\")\n",
    "        v_losses, v_accs, t_losses, t_accs = [], [], [], []\n",
    "        for vloss in savelist[0][1::].split(\",\"):\n",
    "            v_losses.append(float(vloss.strip()))\n",
    "        for vacc in savelist[1][1::].split(\",\"):\n",
    "            v_accs.append(float(vacc.strip()))\n",
    "        for tloss in savelist[2][1::].split(\",\"):\n",
    "            t_losses.append(float(tloss.strip()))\n",
    "        for tacc in savelist[3][1::].split(\",\"):\n",
    "            t_accs.append(float(tacc.strip()))\n",
    "        epochs_done = int(savelist[4])\n",
    "    print(\"Net loaded. \" + str(epochs_done) + \" epochs previously trained.\")\n",
    "    train_net(net=net, epochs=epochs, validation_loss=v_losses, validation_accuracy=v_accs, training_accuracy=t_accs,\n",
    "              training_loss=t_losses, epochs_already_trained=epochs_done)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def test_net(net: nn.Module = None):\n",
    "    if net is None:\n",
    "        try:\n",
    "            net = CNN().to(device)\n",
    "            net.load_state_dict(torch.load(\"./saves/neural_net.pth\"))\n",
    "        except:\n",
    "            return\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # pred_labels = torch.argmax(outputs, axis=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f\"Test Accuracy: {(100 * correct / total):.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def test_net_from_save(save_folder_path: str = \"./saves\"):\n",
    "    net = CNN().to(device)\n",
    "    net.load_state_dict(torch.load(save_folder_path + \"/neural_net.pth\"))\n",
    "    test_net(net)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"256L\",\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"architecture\": \"CNN-256L\",\n",
    "        \"dataset\": \"Flowers102\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"weight_decay\": WEIGHT_DECAY\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv6): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv7): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=131072, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Linear(in_features=1024, out_features=102, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Clear cuda cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create an instance of the CNN and move it to the device\n",
    "cnn = CNN().to(device)\n",
    "print(cnn)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scaiiDdVyNeU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1682812687163,
     "user_tz": -60,
     "elapsed": 229978,
     "user": {
      "displayName": "Faran Lane",
      "userId": "14277689357306891726"
     }
    },
    "outputId": "e088c456-60f2-4075-c8b0-672441162f65",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training.\n",
      "Epoch: [1/20]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [3, 256, 282] at entry 0 and [3, 256, 389] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Start timer\u001B[39;00m\n\u001B[0;32m      3\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtrain_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcnn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_loss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_losses\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_accuracy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_accuracies\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_loss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_losses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m          \u001B[49m\u001B[43mtraining_accuracy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_accuracies\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Stop timer\u001B[39;00m\n\u001B[0;32m      9\u001B[0m end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "Cell \u001B[1;32mIn[18], line 18\u001B[0m, in \u001B[0;36mtrain_net\u001B[1;34m(net, epochs, validation_loss, validation_accuracy, training_loss, training_accuracy, epochs_already_trained)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs_already_trained, epochs):\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 18\u001B[0m     training_loss, training_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_loss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_loss\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m                                                       \u001B[49m\u001B[43mtraining_acc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_accuracy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     validation_loss, validation_accuracy \u001B[38;5;241m=\u001B[39m validate_one_epoch(net, valid_loss\u001B[38;5;241m=\u001B[39mvalidation_loss,\n\u001B[0;32m     21\u001B[0m                                                               valid_acc\u001B[38;5;241m=\u001B[39mvalidation_accuracy)\n\u001B[0;32m     23\u001B[0m     torch\u001B[38;5;241m.\u001B[39msave(net\u001B[38;5;241m.\u001B[39mstate_dict(), SAVE_PATH)\n",
      "Cell \u001B[1;32mIn[16], line 9\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[1;34m(net, training_loss, training_acc, optimizer)\u001B[0m\n\u001B[0;32m      7\u001B[0m correct \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      8\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (images, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m     10\u001B[0m     labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meye(\u001B[38;5;241m102\u001B[39m)[labels]  \u001B[38;5;66;03m# one hot encode\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1344\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1345\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[1;32m-> 1346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1370\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[0;32m   1371\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[1;32m-> 1372\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1373\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\_utils.py:644\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    640\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    641\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[0;32m    642\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[0;32m    643\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m--> 644\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 264, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"C:\\Users\\lanef\\anaconda3\\envs\\INT2\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\", line 162, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [3, 256, 282] at entry 0 and [3, 256, 389] at entry 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training.\")\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "train_net(cnn, EPOCHS, validation_loss=val_losses, validation_accuracy=val_accuracies, training_loss=train_losses,\n",
    "          training_accuracy=train_accuracies)\n",
    "\n",
    "# Stop timer\n",
    "end_time = time.time()\n",
    "print(\"Training Complete in: \" + time.strftime(\"%Hh %Mm %Ss\", time.gmtime(end_time - start_time)))"
   ],
   "id": "scaiiDdVyNeU"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train_net_from_save(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54105,
     "status": "ok",
     "timestamp": 1682812745326,
     "user": {
      "displayName": "Faran Lane",
      "userId": "14277689357306891726"
     },
     "user_tz": -60
    },
    "id": "_XqCOjHvyNeV",
    "outputId": "75a5832f-d2d8-431f-acec-d80e9314cf4b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test the CNN\n",
    "test_net(net=cnn)"
   ],
   "id": "_XqCOjHvyNeV"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjp4w5ifyNeW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 815
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1682812818281,
     "user_tz": -60,
     "elapsed": 2436,
     "user": {
      "displayName": "Faran Lane",
      "userId": "14277689357306891726"
     }
    },
    "outputId": "fc02bdba-e875-4588-af5b-803ca526630d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig = plt.figure(tight_layout=True)\n",
    "gs = grid.GridSpec(nrows=2, ncols=1)\n",
    "\n",
    "\n",
    "# for i in range(len(train_losses)):\n",
    "#     train_losses[i]*=len(all_train_data)\n",
    "def plot_epochs(ax, data: [[int]], ylabel: str, datalabels: [str], cs: [str]):\n",
    "    if len(datalabels) == len(data):\n",
    "        for line in range(len(data)):\n",
    "            ax.plot(data[line], label=datalabels[line], c=cs[line])\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(ylabel)\n",
    "        fig.align_labels()\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "line_labels = [\"Validation\", \"Training\"]\n",
    "colours = [\"forestgreen\", \"royalblue\"]\n",
    "with open(\"./saves/save_data.txt\", \"r\") as savefile:\n",
    "    savelist = savefile.readline().split(\"]\")\n",
    "    v_losses, v_accs, t_losses, t_accs = [], [], [], []\n",
    "    for vloss in savelist[0][1::].split(\",\"):\n",
    "        v_losses.append(float(vloss.strip()))\n",
    "    for vacc in savelist[1][1::].split(\",\"):\n",
    "        v_accs.append(float(vacc.strip()))\n",
    "    for tloss in savelist[2][1::].split(\",\"):\n",
    "        t_losses.append(float(tloss.strip()))\n",
    "    for tacc in savelist[3][1::].split(\",\"):\n",
    "        t_accs.append(float(tacc.strip()))\n",
    "    epochs_done = int(savelist[4])\n",
    "    plot_epochs(fig.add_subplot(gs[0, 0]), [v_accs, t_accs], \"Accuracy\", line_labels, colours)\n",
    "    plot_epochs(fig.add_subplot(gs[1, 0]), [v_losses, t_losses], \"Loss\", line_labels, colours)\n",
    "    plt.show()\n",
    "    fig.savefig(\"figs/training_graph.png\")"
   ],
   "id": "tjp4w5ifyNeW"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}